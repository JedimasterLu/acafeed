{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b92168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title       categories\n",
      "0  Calculation of prompt diphoton production cros...           hep-ph\n",
      "1           Sparsity-certifying Graph Decompositions    math.CO cs.CG\n",
      "2  The evolution of the Earth-Moon system based o...   physics.gen-ph\n",
      "3  A determinant of Stirling cycle numbers counts...          math.CO\n",
      "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  math.CA math.FA\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = os.path.expanduser(\"~/.cache/kagglehub/datasets/Cornell-University/arxiv/versions/259/arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "# åˆ†å—è¯»å–ï¼ŒåŠ å¿«é€Ÿåº¦ã€èŠ‚çœå†…å­˜\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=100000)\n",
    "\n",
    "dfs = []\n",
    "for chunk in chunks:\n",
    "    dfs.append(chunk[[\"title\", \"categories\"]])   # åªä¿ç•™æ„Ÿå…´è¶£çš„åˆ—\n",
    "\n",
    "df_small = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b0651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title         categories\n",
      "0   Calculation of prompt diphoton production cros...             hep-ph\n",
      "1            Sparsity-certifying Graph Decompositions      math.CO cs.CG\n",
      "2   The evolution of the Earth-Moon system based o...     physics.gen-ph\n",
      "3   A determinant of Stirling cycle numbers counts...            math.CO\n",
      "4   From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...    math.CA math.FA\n",
      "5   Bosonic characters of atomic Cooper pairs acro...  cond-mat.mes-hall\n",
      "6   Polymer Quantum Mechanics and its Continuum Limit              gr-qc\n",
      "7   Numerical solution of shock and ramp compressi...  cond-mat.mtrl-sci\n",
      "8   The Spitzer c2d Survey of Large, Nearby, Inste...           astro-ph\n",
      "9   Partial cubes: structures, characterizations, ...            math.CO\n",
      "10  Computing genus 2 Hilbert-Siegel modular forms...    math.NT math.AG\n",
      "11  Distribution of integral Fourier Coefficients ...            math.NT\n",
      "12  $p$-adic Limit of Weakly Holomorphic Modular F...            math.NT\n",
      "13             Iterated integral and the loop product    math.CA math.AT\n",
      "14  Fermionic superstring loop amplitudes in the p...             hep-th\n",
      "15                 Lifetime of doubly charmed baryons             hep-ph\n",
      "16  Spectroscopic Observations of the Intermediate...           astro-ph\n",
      "17    In quest of a generalized Callias index theorem             hep-th\n",
      "18  Approximation for extinction probability of th...    math.PR math.AG\n",
      "19  Measurement of the Hadronic Form Factor in D0 ...             hep-ex\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_small.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7b726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title             Category\n",
      "0   Calculation of prompt diphoton production cros...             [hep-ph]\n",
      "1            Sparsity-certifying Graph Decompositions     [math.CO, cs.CG]\n",
      "2   The evolution of the Earth-Moon system based o...     [physics.gen-ph]\n",
      "3   A determinant of Stirling cycle numbers counts...            [math.CO]\n",
      "4   From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   [math.CA, math.FA]\n",
      "5   Bosonic characters of atomic Cooper pairs acro...  [cond-mat.mes-hall]\n",
      "6   Polymer Quantum Mechanics and its Continuum Limit              [gr-qc]\n",
      "7   Numerical solution of shock and ramp compressi...  [cond-mat.mtrl-sci]\n",
      "8   The Spitzer c2d Survey of Large, Nearby, Inste...           [astro-ph]\n",
      "9   Partial cubes: structures, characterizations, ...            [math.CO]\n",
      "10  Computing genus 2 Hilbert-Siegel modular forms...   [math.NT, math.AG]\n",
      "11  Distribution of integral Fourier Coefficients ...            [math.NT]\n",
      "12  $p$-adic Limit of Weakly Holomorphic Modular F...            [math.NT]\n",
      "13             Iterated integral and the loop product   [math.CA, math.AT]\n",
      "14  Fermionic superstring loop amplitudes in the p...             [hep-th]\n",
      "15                 Lifetime of doubly charmed baryons             [hep-ph]\n",
      "16  Spectroscopic Observations of the Intermediate...           [astro-ph]\n",
      "17    In quest of a generalized Callias index theorem             [hep-th]\n",
      "18  Approximation for extinction probability of th...   [math.PR, math.AG]\n",
      "19  Measurement of the Hadronic Form Factor in D0 ...             [hep-ex]\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "raw_df = df_small.rename(columns={\"categories\": \"Category\", \"title\": \"Title\"})\n",
    "# Convert categories from string to list\n",
    "raw_df[\"Category\"] = raw_df[\"Category\"].str.split(' ')\n",
    "# Strip whitespace from titles\n",
    "raw_df[\"Title\"] = raw_df[\"Title\"].str.strip()\n",
    "\n",
    "print(raw_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34a09b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs.LG                 240606\n",
      "hep-ph                191472\n",
      "hep-th                177619\n",
      "cs.CV                 172242\n",
      "quant-ph              169738\n",
      "cs.AI                 149866\n",
      "gr-qc                 117367\n",
      "astro-ph              105380\n",
      "cond-mat.mtrl-sci     103831\n",
      "cond-mat.mes-hall      98127\n",
      "cs.CL                  96563\n",
      "math-ph                86649\n",
      "math.MP                86649\n",
      "cond-mat.str-el        80077\n",
      "cond-mat.stat-mech     78891\n",
      "math.CO                74424\n",
      "astro-ph.CO            74148\n",
      "stat.ML                74107\n",
      "astro-ph.GA            73446\n",
      "math.AP                70186\n",
      "Name: count, dtype: int64\n",
      "Total unique categories: 176\n"
     ]
    }
   ],
   "source": [
    "# Calculate the catrgory and its frequency\n",
    "# Concactenate all categories into a single list\n",
    "all_categories = [category for sublist in raw_df[\"Category\"] for category in sublist]\n",
    "# Calculate frequency\n",
    "category_counts = pd.Series(all_categories).value_counts()\n",
    "print(category_counts.head(20))\n",
    "print(f\"Total unique categories: {len(category_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "873a25fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2734420, Testing samples: 143917\n",
      "Number of labels: 176\n"
     ]
    }
   ],
   "source": [
    "# æ ‡ç­¾æ˜ å°„ä¸æ•°æ®é›†æ‹†åˆ†ï¼ˆè¿‡æ»¤å•æ ·æœ¬ç±»åˆ«åï¼‰\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filtered_categories = category_counts[category_counts > 1].index.tolist()\n",
    "filtered_df = raw_df[raw_df[\"Category\"].apply(lambda cats: any(cat in filtered_categories for cat in cats))].reset_index(drop=True)\n",
    "category_to_id = {cat: idx for idx, cat in enumerate(filtered_categories)}\n",
    "\n",
    "# ä¸ºå¤šæ ‡ç­¾åˆ†ç±»åˆ›å»º one-hot ç¼–ç æ ‡ç­¾\n",
    "import numpy as np\n",
    "def encode_labels(cats):\n",
    "    labels = np.zeros(len(category_to_id), dtype=float)\n",
    "    for cat in cats:\n",
    "        if cat in category_to_id:\n",
    "            labels[category_to_id[cat]] = 1.0\n",
    "    return labels\n",
    "\n",
    "filtered_df[\"label\"] = filtered_df[\"Category\"].apply(encode_labels)\n",
    "train_df, test_df = train_test_split(filtered_df, test_size=0.05, random_state=42)\n",
    "print(f\"Training samples: {len(train_df)}, Testing samples: {len(test_df)}\")\n",
    "print(f\"Number of labels: {len(category_to_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3871052e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1250e9628d844ab9bfc98ccc2e649ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fe1b2633f64df68d7de04f1ab5850a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†: 10000 æ ·æœ¬, æµ‹è¯•é›†: 500 æ ·æœ¬\n",
      "åºåˆ—æœ€å¤§é•¿åº¦: 64\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10000\n",
      "}) Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# è½¬æ¢ä¸º HuggingFace Datasets å¹¶åˆ†è¯\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# å‡å°‘è®­ç»ƒæ ·æœ¬æ•°é‡ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦\n",
    "# åŸæ¥ï¼š100,000 æ ·æœ¬ -> ç°åœ¨ï¼š10,000 æ ·æœ¬ (10å€åŠ é€Ÿ)\n",
    "train_ds = Dataset.from_pandas(train_df.sample(n=10000, random_state=42).reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.sample(n=500, random_state=42).reset_index(drop=True))\n",
    "\n",
    "# å‡å°‘åºåˆ—é•¿åº¦ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦\n",
    "# åŸæ¥ï¼š128 -> ç°åœ¨ï¼š64 (çº¦2å€åŠ é€Ÿ)\n",
    "max_length = 64\n",
    "\n",
    "tokenizer_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized = tokenizer(batch['Title'], truncation=True, padding='max_length', max_length=max_length)\n",
    "    # å°† label é‡å‘½åä¸º labelsï¼ˆå¤æ•°ï¼‰ï¼Œè¿™æ˜¯ Hugging Face æ¨¡å‹æœŸæœ›çš„å­—æ®µå\n",
    "    tokenized['labels'] = batch['label']\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = train_ds.map(tokenize, batched=True)\n",
    "test_tokenized = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "cols = ['input_ids','attention_mask','labels']\n",
    "train_tokenized = train_tokenized.remove_columns([c for c in train_tokenized.column_names if c not in cols])\n",
    "test_tokenized = test_tokenized.remove_columns([c for c in test_tokenized.column_names if c not in cols])\n",
    "\n",
    "train_tokenized.set_format(type='torch')\n",
    "test_tokenized.set_format(type='torch')\n",
    "\n",
    "print(f'è®­ç»ƒé›†: {len(train_tokenized)} æ ·æœ¬, æµ‹è¯•é›†: {len(test_tokenized)} æ ·æœ¬')\n",
    "print(f'åºåˆ—æœ€å¤§é•¿åº¦: {max_length}')\n",
    "print(train_tokenized, test_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcce6453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹å·²åˆå§‹åŒ–ï¼Œæ ‡ç­¾æ•°: 176\n",
      "é—®é¢˜ç±»å‹: multi_label_classification\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨åŸºç¡€ distilbert è€Œé SST-2 å¤´ï¼Œé…ç½®ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ï¼‰\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "base_model_name = 'distilbert/distilbert-base-uncased'\n",
    "\n",
    "config = AutoConfig.from_pretrained(base_model_name)\n",
    "# è®¾ç½®æ ‡ç­¾æ•°é‡å’Œé—®é¢˜ç±»å‹ä¸ºå¤šæ ‡ç­¾åˆ†ç±»\n",
    "config.num_labels = len(category_to_id)\n",
    "config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True  # å…è®¸é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´\n",
    ")\n",
    "\n",
    "print('æ¨¡å‹å·²åˆå§‹åŒ–ï¼Œæ ‡ç­¾æ•°:', model.config.num_labels)\n",
    "print('é—®é¢˜ç±»å‹:', model.config.problem_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7df7dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer å·²åˆå§‹åŒ–,å‡†å¤‡å¼€å§‹è®­ç»ƒ\n",
      "è®­ç»ƒé…ç½®: batch_size=256, max_length=64\n",
      "é¢„è®¡è®­ç»ƒæ­¥æ•°: 39 æ­¥/epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jylu/Projects/acafeed/.venv/lib/python3.14/site-packages/transformers/training_args.py:2301: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒè®¾ç½®ä¸ Trainer åˆå§‹åŒ– (åªéœ€è¿è¡Œä¸€æ¬¡)\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "output_dir = 'model/checkpoint'\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = sigmoid(logits)\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    \n",
    "    # è®¡ç®—å¤šæ ‡ç­¾æŒ‡æ ‡\n",
    "    # æ ·æœ¬çº§å‡†ç¡®ç‡ï¼šé¢„æµ‹å®Œå…¨åŒ¹é…çš„æ¯”ä¾‹\n",
    "    exact_match = (preds == labels).all(axis=1).mean()\n",
    "    \n",
    "    # å¾®å¹³å‡å’Œå®å¹³å‡ F1\n",
    "    f1_micro = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'exact_match': exact_match,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=256,  # å¢å¤§ batch size (128 -> 256) åŠ é€Ÿè®­ç»ƒ\n",
    "    per_device_eval_batch_size=256,\n",
    "    num_train_epochs=1,  # æ¯æ¬¡åªè·‘ 1 ä¸ª epoch,å¯ä»¥æ‰‹åŠ¨å¤šæ¬¡è¿è¡Œ\n",
    "    learning_rate=5e-5,\n",
    "    eval_strategy='epoch',  # æ¯ä¸ª epoch ç»“æŸåè¯„ä¼°\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=5,  # å‡å°‘æ—¥å¿—é¢‘ç‡ (10 -> 5)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_micro',\n",
    "    save_total_limit=2,\n",
    "    report_to='none',\n",
    "    use_mps_device=True,  # ä½¿ç”¨ Apple Silicon MPS\n",
    "    fp16=False,  # MPS ä¸æ”¯æŒ fp16ï¼Œä½†å¯ä»¥è€ƒè™‘ä½¿ç”¨ bf16\n",
    "    gradient_accumulation_steps=1,  # ä¿æŒé»˜è®¤\n",
    "    dataloader_num_workers=0,  # MPS ä½¿ç”¨ 0 æ¯”è¾ƒç¨³å®š\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print('Trainer å·²åˆå§‹åŒ–,å‡†å¤‡å¼€å§‹è®­ç»ƒ')\n",
    "print(f'è®­ç»ƒé…ç½®: batch_size={training_args.per_device_train_batch_size}, max_length={max_length}')\n",
    "print(f'é¢„è®¡è®­ç»ƒæ­¥æ•°: {len(train_tokenized) // training_args.per_device_train_batch_size} æ­¥/epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "616b3992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jylu/Projects/acafeed/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.051604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=0.054133594036102295, metrics={'train_runtime': 79.5518, 'train_samples_per_second': 125.704, 'train_steps_per_second': 0.503, 'total_flos': 166098063360000.0, 'train_loss': 0.054133594036102295, 'epoch': 1.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ‰§è¡Œè®­ç»ƒ (å¯é‡å¤è¿è¡Œæ­¤å•å…ƒæ ¼è¿›è¡Œå¤šæ¬¡ epoch è®­ç»ƒ)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbed12cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jylu/Projects/acafeed/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 05:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 0.09000758081674576, 'eval_exact_match': 0.0, 'eval_f1_micro': 0.0, 'eval_f1_macro': 0.0, 'eval_runtime': 1.7333, 'eval_samples_per_second': 288.467, 'eval_steps_per_second': 1.154, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°ä¸åˆ†ç±»æŠ¥å‘Š\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print('Eval metrics:', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1974a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹è®¾å¤‡: mps:0\n",
      "\n",
      "================================================================================\n",
      "Title: Calculation of prompt diphoton production cross sections at Tevatron and\n",
      "  LHC energies\n",
      "é¢„æµ‹æ ‡ç­¾ (>0.5): 0 ä¸ª\n",
      "\n",
      "Top 5 æ ‡ç­¾:\n",
      "  - cs.LG: 0.073\n",
      "  - hep-ph: 0.056\n",
      "  - hep-th: 0.052\n",
      "  - quant-ph: 0.050\n",
      "  - cs.CV: 0.049\n",
      "\n",
      "================================================================================\n",
      "Title: Computing genus 2 Hilbert-Siegel modular forms over $\\Q(\\sqrt{5})$ via\n",
      "  the Jacquet-Langlands corre\n",
      "é¢„æµ‹æ ‡ç­¾ (>0.5): 0 ä¸ª\n",
      "\n",
      "Top 5 æ ‡ç­¾:\n",
      "  - cs.LG: 0.073\n",
      "  - hep-ph: 0.055\n",
      "  - hep-th: 0.052\n",
      "  - quant-ph: 0.051\n",
      "  - cs.CV: 0.049\n",
      "\n",
      "================================================================================\n",
      "Title: The birth of string theory\n",
      "é¢„æµ‹æ ‡ç­¾ (>0.5): 0 ä¸ª\n",
      "\n",
      "Top 5 æ ‡ç­¾:\n",
      "  - cs.LG: 0.074\n",
      "  - hep-ph: 0.056\n",
      "  - hep-th: 0.053\n",
      "  - quant-ph: 0.051\n",
      "  - cs.CV: 0.049\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# åˆ›å»º id åˆ°æ ‡ç­¾çš„æ˜ å°„\n",
    "id2label = {v: k for k, v in category_to_id.items()}\n",
    "\n",
    "# æ¨ç†ç¤ºä¾‹\n",
    "sample_titles = [\n",
    "    raw_df.iloc[0]['Title'],\n",
    "    raw_df.iloc[10]['Title'],\n",
    "    raw_df.iloc[100]['Title']\n",
    "]\n",
    "\n",
    "# è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡\n",
    "device = model.device\n",
    "print(f'æ¨¡å‹è®¾å¤‡: {device}')\n",
    "\n",
    "model.eval()\n",
    "# DistilBERT ä¸æ”¯æŒ token_type_idsï¼Œéœ€è¦ç§»é™¤\n",
    "encoded = tokenizer(sample_titles, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "# ç§»é™¤ token_type_idsï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "if 'token_type_ids' in encoded:\n",
    "    encoded.pop('token_type_ids')\n",
    "\n",
    "# å°†è¾“å…¥å¼ é‡ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ï¼ˆMPSï¼‰\n",
    "encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "    # å¤šæ ‡ç­¾åˆ†ç±»ä½¿ç”¨ sigmoid è€Œé softmax\n",
    "    probs = torch.sigmoid(outputs.logits)\n",
    "    # å°†ç»“æœç§»å› CPU è¿›è¡Œåå¤„ç†\n",
    "    probs = probs.cpu()\n",
    "\n",
    "for title, prob in zip(sample_titles, probs):\n",
    "    # è·å–æ¦‚ç‡ > 0.5 çš„æ‰€æœ‰æ ‡ç­¾\n",
    "    pred_indices = (prob > 0.5).nonzero(as_tuple=True)[0].tolist()\n",
    "    pred_labels = [(id2label[i], float(prob[i])) for i in pred_indices]\n",
    "    \n",
    "    # è·å– Top 5 æœ€é«˜æ¦‚ç‡çš„æ ‡ç­¾\n",
    "    top5 = sorted([(id2label[i], float(p)) for i, p in enumerate(prob)], key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print('Title:', title[:100])\n",
    "    print(f'é¢„æµ‹æ ‡ç­¾ (>0.5): {len(pred_labels)} ä¸ª')\n",
    "    for label, score in pred_labels:\n",
    "        print(f'  - {label}: {score:.3f}')\n",
    "    print('\\nTop 5 æ ‡ç­¾:')\n",
    "    for label, score in top5:\n",
    "        print(f'  - {label}: {score:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acafeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
