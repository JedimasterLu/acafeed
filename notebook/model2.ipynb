{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2687f820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Checkpoint dir: /home/ljy/projects/acafeed/checkpoint\n"
     ]
    }
   ],
   "source": [
    "# 导入库与基础配置\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 设备与随机种子\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "SEED = 302\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# checkpoint 目录\n",
    "CHECKPOINT_DIR = Path(\"checkpoint\")\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LAST_CKPT = CHECKPOINT_DIR / \"model1_last.pt\"\n",
    "BEST_CKPT = CHECKPOINT_DIR / \"model1_best.pt\"\n",
    "\n",
    "print(\"Checkpoint dir:\", CHECKPOINT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "794654a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ljy/.cache/kagglehub/datasets/Cornell-University/arxiv/versions/260\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81f07694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2740089, Testing samples: 144216\n",
      "Number of labels: 174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bac4dc9a4a84631b7c7fbad3af9def9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2876e0d73c244275a705295772cdae3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 50000 样本, 测试集: 1000 样本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: BertForSequenceClassification\n",
      "Num labels: 174\n",
      "Problem type: multi_label_classification\n"
     ]
    }
   ],
   "source": [
    "# 定义数据集与多标签模型（Model1）\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# 为了复用原始数据，这里从现有的 arxiv JSON 构建与 model.ipynb 一致的数据流程\n",
    "file_path = os.path.expanduser(\"~/.cache/kagglehub/datasets/Cornell-University/arxiv/versions/260/arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=100000)\n",
    "dfs = []\n",
    "for chunk in chunks:\n",
    "    dfs.append(chunk[[\"title\", \"categories\"]])\n",
    "\n",
    "df_small = pd.concat(dfs, ignore_index=True)\n",
    "raw_df = df_small.rename(columns={\"categories\": \"Category\", \"title\": \"Title\"})\n",
    "raw_df[\"Category\"] = raw_df[\"Category\"].str.split(\" \")\n",
    "raw_df[\"Title\"] = raw_df[\"Title\"].str.strip()\n",
    "\n",
    "# 统计类别并过滤低频类别\n",
    "all_categories = [c for sub in raw_df[\"Category\"] for c in sub]\n",
    "category_counts = pd.Series(all_categories).value_counts()\n",
    "filtered_categories = category_counts[category_counts >= 20].index.tolist()\n",
    "\n",
    "category_to_id = {cat: idx for idx, cat in enumerate(filtered_categories)}\n",
    "label2id = category_to_id\n",
    "id2label = {v: k for k, v in category_to_id.items()}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def encode_labels(cats):\n",
    "    labels = np.zeros(len(category_to_id), dtype=float)\n",
    "    for cat in cats:\n",
    "        if cat in category_to_id:\n",
    "            labels[category_to_id[cat]] = 1.0\n",
    "    return labels\n",
    "\n",
    "filtered_df = raw_df[raw_df[\"Category\"].apply(lambda cats: any(cat in filtered_categories for cat in cats))].reset_index(drop=True)\n",
    "filtered_df[\"label\"] = filtered_df[\"Category\"].apply(encode_labels)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(filtered_df, test_size=0.05, random_state=SEED)\n",
    "print(f\"Training samples: {len(train_df)}, Testing samples: {len(test_df)}\")\n",
    "print(f\"Number of labels: {len(category_to_id)}\")\n",
    "\n",
    "# 转为 HF Dataset\n",
    "# 限制个数\n",
    "TRAINNUM = 50000\n",
    "TESTNUM = 1000\n",
    "# Randomly select a subset for quicker testing\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True).sample(TRAINNUM, random_state=SEED)).shuffle(seed=SEED)\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True).sample(TESTNUM, random_state=SEED)).shuffle(seed=SEED)\n",
    "\n",
    "max_length = 64\n",
    "# 与模型保持一致的分词器：SCIBERT（SciVocab, uncased）\n",
    "tokenizer_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized = tokenizer(batch[\"Title\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    tokenized[\"labels\"] = batch[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = train_ds.map(tokenize, batched=True)\n",
    "test_tokenized = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_tokenized = train_tokenized.remove_columns([c for c in train_tokenized.column_names if c not in cols])\n",
    "test_tokenized = test_tokenized.remove_columns([c for c in test_tokenized.column_names if c not in cols])\n",
    "\n",
    "train_tokenized.set_format(type=\"torch\")\n",
    "test_tokenized.set_format(type=\"torch\")\n",
    "\n",
    "print(f\"训练集: {len(train_tokenized)} 样本, 测试集: {len(test_tokenized)} 样本\")\n",
    "\n",
    "# 定义模型（多标签 DistilBERT）\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "base_model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "config = AutoConfig.from_pretrained(base_model_name)\n",
    "config.num_labels = len(category_to_id)\n",
    "config.problem_type = \"multi_label_classification\"\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "# 增强正则化以缓解过拟合\n",
    "config.hidden_dropout_prob = 0.2\n",
    "config.attention_probs_dropout_prob = 0.1\n",
    "# 部分模型使用 classifier_dropout（若不支持将被忽略）\n",
    "setattr(config, \"classifier_dropout\", 0.2)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model initialized:\", model.__class__.__name__)\n",
    "print(\"Num labels:\", model.config.num_labels)\n",
    "print(\"Problem type:\", model.config.problem_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03a93a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing checkpoint found, skip dummy save.\n"
     ]
    }
   ],
   "source": [
    "# 实现 checkpoint 保存与加载工具函数\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def save_checkpoint(state: Dict[str, Any], filename: Path):\n",
    "    \"\"\"保存训练状态到指定文件。\n",
    "\n",
    "    state 示例:\n",
    "    {\n",
    "        'epoch': int,\n",
    "        'global_step': int,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'best_metric': float\n",
    "    }\n",
    "    \"\"\"\n",
    "    filename = Path(filename)\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename: Path):\n",
    "    \"\"\"从指定文件加载训练状态，返回 (start_epoch, global_step, best_metric)。\"\"\"\n",
    "    filename = Path(filename)\n",
    "    if not filename.exists():\n",
    "        print(f\"No checkpoint found at {filename}, start from scratch.\")\n",
    "        return 0, 0, None\n",
    "\n",
    "    ckpt = torch.load(filename, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    if optimizer is not None and \"optimizer_state\" in ckpt:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    start_epoch = ckpt.get(\"epoch\", 0)\n",
    "    global_step = ckpt.get(\"global_step\", 0)\n",
    "    best_metric = ckpt.get(\"best_metric\", None)\n",
    "    print(f\"Loaded checkpoint from {filename}: epoch={start_epoch}, global_step={global_step}, best_metric={best_metric}\")\n",
    "    return start_epoch, global_step, best_metric\n",
    "\n",
    "\n",
    "# 简单测试：仅在第一次运行时保存一个空的 state 示例\n",
    "if not LAST_CKPT.exists():\n",
    "    dummy_state = {\n",
    "        \"epoch\": 0,\n",
    "        \"global_step\": 0,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"best_metric\": None,\n",
    "    }\n",
    "    save_checkpoint(dummy_state, LAST_CKPT)\n",
    "else:\n",
    "    print(\"Existing checkpoint found, skip dummy save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ccc2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from checkpoint/model1_last.pt: epoch=10, global_step=5474, best_metric=0.5403552508569648\n",
      "Trainer initialized. start_epoch= 10 global_step= 5474 best_metric= 0.5403552508569648\n"
     ]
    }
   ],
   "source": [
    "# 训练设置与 Trainer 初始化（集成 checkpoint 保存与自动加载）\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 指标阈值\n",
    "def sigmoid_np(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "THRESH = 0.4  # 多标签预测阈值\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = sigmoid_np(logits)\n",
    "    preds = (probs > THRESH).astype(int)\n",
    "\n",
    "    exact_match = (preds == labels).all(axis=1).mean()\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": exact_match,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "    }\n",
    "\n",
    "\n",
    "output_dir = CHECKPOINT_DIR  # 直接复用 checkpoint 目录\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\",  # 正确字段名\n",
    "    save_strategy=\"epoch\",         # 每个 epoch 保存以便选择最优\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_micro\",\n",
    "    greater_is_better=True,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=20,\n",
    "    report_to=\"none\",\n",
    "    use_mps_device=(device.type == \"mps\"),\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "# 如果需要手动优化器，可保留；Trainer 内部也会创建优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "# 自动加载最新 checkpoint（如果存在）\n",
    "start_epoch, global_step, best_metric = load_checkpoint(model, optimizer, LAST_CKPT)\n",
    "if best_metric is None:\n",
    "    best_metric = 0.0\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# 加入早停，进一步缓解过拟合\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=1e-4))\n",
    "\n",
    "print(\"Trainer initialized. start_epoch=\", start_epoch, \"global_step=\", global_step, \"best_metric=\", best_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d13f0d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training (will evaluate each epoch, early stopping enabled)\n",
      "Resuming from HF checkpoint: checkpoint/checkpoint-5474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7820' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7820/7820 08:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.556888</td>\n",
       "      <td>0.319164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.557556</td>\n",
       "      <td>0.320219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.023514</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.561826</td>\n",
       "      <td>0.324386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 0.023513667285442352, 'eval_exact_match': 0.292, 'eval_f1_micro': 0.5618262523779328, 'eval_f1_macro': 0.32438641061709894, 'eval_runtime': 1.2133, 'eval_samples_per_second': 824.209, 'eval_steps_per_second': 13.187, 'epoch': 10.0}\n",
      "Checkpoint saved to checkpoint/model1_last.pt\n",
      "Checkpoint saved to checkpoint/model1_last.pt\n",
      "Checkpoint saved to checkpoint/model1_best.pt\n",
      "Training finished. Best eval_f1_micro= 0.5618262523779328\n",
      "Checkpoint saved to checkpoint/model1_best.pt\n",
      "Training finished. Best eval_f1_micro= 0.5618262523779328\n"
     ]
    }
   ],
   "source": [
    "# 主训练（使用 Trainer 自带循环 + 早停 + 最优模型自动加载）\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "print(f\"Start training (will evaluate each epoch, early stopping enabled)\")\n",
    "\n",
    "# 仅从 HuggingFace 的 checkpoint-* 目录续训，避免使用 .pt 文件导致报错\n",
    "last_hf_ckpt = None\n",
    "try:\n",
    "    last_hf_ckpt = get_last_checkpoint(str(output_dir))\n",
    "except Exception:\n",
    "    last_hf_ckpt = None\n",
    "\n",
    "if last_hf_ckpt:\n",
    "    print(f\"Resuming from HF checkpoint: {last_hf_ckpt}\")\n",
    "    train_result = trainer.train(resume_from_checkpoint=last_hf_ckpt)\n",
    "else:\n",
    "    train_result = trainer.train()\n",
    "\n",
    "# 训练结束后，已自动将最优模型权重加载到 model（load_best_model_at_end=True）\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Eval metrics:\", eval_metrics)\n",
    "\n",
    "# 使用自定义保存：保存最后与最优到我们的目录\n",
    "state = {\n",
    "    \"epoch\": int(training_args.num_train_epochs),\n",
    "    \"global_step\": int(train_result.global_step),\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"best_metric\": float(eval_metrics.get(\"eval_f1_micro\", 0.0)),\n",
    "}\n",
    "save_checkpoint(state, LAST_CKPT)\n",
    "# 当前 model 为最优权重，另存一份到 BEST_CKPT 便于下游推理\n",
    "save_checkpoint(state, BEST_CKPT)\n",
    "\n",
    "print(\"Training finished. Best eval_f1_micro=\", eval_metrics.get(\"eval_f1_micro\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea125e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from checkpoint/model1_best.pt: epoch=10, global_step=7820, best_metric=0.5618262523779328\n",
      "\n",
      "================================================================================\n",
      "Title: Morphing the left atrium geometry: The role of the pulmonary veins on\n",
      "  flow patterns and thrombus f\n",
      "预测标签 (>0.4): 2 个\n",
      "  - physics.bio-ph: 0.513\n",
      "  - physics.med-ph: 0.429\n",
      "真实标签: ['physics.med-ph', 'physics.flu-dyn']\n",
      "\n",
      "Top 5 标签:\n",
      "  - physics.bio-ph: 0.513\n",
      "  - physics.med-ph: 0.429\n",
      "  - physics.flu-dyn: 0.268\n",
      "  - q-bio.TO: 0.262\n",
      "  - q-bio.CB: 0.096\n",
      "\n",
      "================================================================================\n",
      "Title: Upon the existence of short-time approximations of any polynomial order\n",
      "  for the computation of den\n",
      "预测标签 (>0.4): 1 个\n",
      "  - math.NA: 0.735\n",
      "真实标签: ['physics.med-ph', 'physics.flu-dyn']\n",
      "\n",
      "Top 5 标签:\n",
      "  - physics.bio-ph: 0.513\n",
      "  - physics.med-ph: 0.429\n",
      "  - physics.flu-dyn: 0.268\n",
      "  - q-bio.TO: 0.262\n",
      "  - q-bio.CB: 0.096\n",
      "\n",
      "================================================================================\n",
      "Title: Upon the existence of short-time approximations of any polynomial order\n",
      "  for the computation of den\n",
      "预测标签 (>0.4): 1 个\n",
      "  - math.NA: 0.735\n",
      "真实标签: ['math-ph', 'cond-mat.stat-mech', 'math.MP', 'physics.chem-ph']\n",
      "\n",
      "Top 5 标签:\n",
      "  - math.NA: 0.735\n",
      "  - cs.NA: 0.394\n",
      "  - physics.comp-ph: 0.299\n",
      "  - quant-ph: 0.103\n",
      "  - physics.chem-ph: 0.029\n",
      "\n",
      "================================================================================\n",
      "Title: Generalization of Urban Wind Environment Using Fourier Neural Operator\n",
      "  Across Different Wind Direc\n",
      "预测标签 (>0.4): 1 个\n",
      "  - cs.LG: 0.814\n",
      "真实标签: ['cs.LG', 'cs.CE', 'physics.flu-dyn']\n",
      "\n",
      "Top 5 标签:\n",
      "  - cs.LG: 0.814\n",
      "  - physics.ao-ph: 0.310\n",
      "  - stat.ML: 0.191\n",
      "  - physics.geo-ph: 0.152\n",
      "  - stat.AP: 0.136\n",
      "真实标签: ['math-ph', 'cond-mat.stat-mech', 'math.MP', 'physics.chem-ph']\n",
      "\n",
      "Top 5 标签:\n",
      "  - math.NA: 0.735\n",
      "  - cs.NA: 0.394\n",
      "  - physics.comp-ph: 0.299\n",
      "  - quant-ph: 0.103\n",
      "  - physics.chem-ph: 0.029\n",
      "\n",
      "================================================================================\n",
      "Title: Generalization of Urban Wind Environment Using Fourier Neural Operator\n",
      "  Across Different Wind Direc\n",
      "预测标签 (>0.4): 1 个\n",
      "  - cs.LG: 0.814\n",
      "真实标签: ['cs.LG', 'cs.CE', 'physics.flu-dyn']\n",
      "\n",
      "Top 5 标签:\n",
      "  - cs.LG: 0.814\n",
      "  - physics.ao-ph: 0.310\n",
      "  - stat.ML: 0.191\n",
      "  - physics.geo-ph: 0.152\n",
      "  - stat.AP: 0.136\n"
     ]
    }
   ],
   "source": [
    "# 推理与多标签预测示例（使用 best 或 last checkpoint）\n",
    "\n",
    "# 优先加载最佳 checkpoint\n",
    "if BEST_CKPT.exists():\n",
    "    load_checkpoint(model, optimizer=None, filename=BEST_CKPT)\n",
    "else:\n",
    "    load_checkpoint(model, optimizer=None, filename=LAST_CKPT)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sample_index = np.random.randint(0, len(filtered_df) - 1, 3)\n",
    "sample_titles = [\n",
    "    filtered_df.iloc[sample_index[0]][\"Title\"],\n",
    "    filtered_df.iloc[sample_index[1]][\"Title\"],\n",
    "    filtered_df.iloc[sample_index[2]][\"Title\"],\n",
    "]\n",
    "\n",
    "encoded = tokenizer(sample_titles, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "    probs = torch.sigmoid(outputs.logits).cpu()\n",
    "\n",
    "for title, prob in zip(sample_titles, probs):\n",
    "    pred_indices = (prob > THRESH).nonzero(as_tuple=True)[0].tolist()\n",
    "    pred_labels = [(id2label[i], float(prob[i])) for i in pred_indices]\n",
    "    top5 = sorted([(id2label[i], float(p)) for i, p in enumerate(prob)], key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Title:\", title[:100])\n",
    "    print(f\"预测标签 (>{THRESH}): {len(pred_labels)} 个\")\n",
    "    for label, score in pred_labels:\n",
    "        print(f\"  - {label}: {score:.3f}\")\n",
    "    print(f\"真实标签: {filtered_df[filtered_df['Title'] == title]['Category'].values[0]}\")\n",
    "    print(\"\\nTop 5 标签:\")\n",
    "    for label, score in top5:\n",
    "        print(f\"  - {label}: {score:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acafeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
